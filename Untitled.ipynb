{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#CrossVal\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold as KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit as SSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#GridSearch\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#ensembles\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "#text stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Just the preprocessing funtion. Play around with various features\n",
    "#I'm not using all the features yet\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = df.drop(['id', 'tid', 'date','dept' ], axis = 1)\n",
    "#      #One-hot forcredit\n",
    "#     df = df.join(pd.get_dummies(df.forcredit, prefix= 'forcredit'))\n",
    "    df = df.drop('forcredit', axis =1 )\n",
    "#     #One-hot attendance\n",
    "#     df = df.join(pd.get_dummies(df.attendance, prefix= 'attendance'))\n",
    "    df = df.drop('attendance', axis = 1)\n",
    "#      #One-hot textbookuse\n",
    "    df = df.join(pd.get_dummies(df.textbookuse, prefix= 'textbk'))\n",
    "    df = df.drop('textbookuse' , axis =1)\n",
    "#      #One-hot interest\n",
    "    df = df.join(pd.get_dummies(df.interest, prefix= 'interest'))\n",
    "    df = df.drop('interest', axis =1)\n",
    "#      #One-hot grade\n",
    "    df = df.join(pd.get_dummies(df.grade, prefix= 'grade'))\n",
    "    df = df.drop('grade', axis =1)\n",
    "#      #One-hot profgender\n",
    "    df = df.join(pd.get_dummies(df.profgender, prefix= 'profgender'))\n",
    "    df = df.drop('profgender', axis =1)\n",
    "#      #One-hot profhotness\n",
    "    df = df.join(pd.get_dummies(df.profhotness, prefix= 'profhotness'))\n",
    "    df = df.drop('profhotness', axis =1)\n",
    "#      #One-hot online\n",
    "    df = df.join(pd.get_dummies(df.online, prefix= 'online'))\n",
    "    df = df.drop('online', axis =1)\n",
    "     #One-hot tags\n",
    "    df.tags = df.tags.apply(lambda x: '*'.join(eval(x)))\n",
    "    df = df.join( df.tags.str.get_dummies(sep='*'))\n",
    "    df = df.drop('tags', axis =1)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: 0.573069770675\n",
      "MSE: 2.97629965055\n"
     ]
    }
   ],
   "source": [
    "#CrossVal - I'm splitting the data into train and test, running the\n",
    "# preprocessing on all the training sets. Change the second number\n",
    "#in SSplit to control tehe number of iteration of crossval it should do\n",
    "#Try differnet models  -LR seems best for now.\n",
    "\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "Y = data['quality']\n",
    "sss = SSplit(Y,1 ,test_size=0.4, random_state=7)\n",
    "\n",
    "for train_index, test_index in sss:\n",
    "    X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    dropcolnames = []\n",
    "    dropcolnames.extend(['quality', \n",
    "                         'helpfulness', 'clarity', 'easiness'])\n",
    "\n",
    "    X_train = X_train.drop(dropcolnames,axis = 1)\n",
    "    X_test = X_test.drop(dropcolnames, axis = 1)\n",
    "\n",
    "    X_train = preprocessing(X_train)  \n",
    "\n",
    "\n",
    "    # #tfidf on train\n",
    "    tfidf2 = TfidfVectorizer(token_pattern=r'(?u)\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                             max_features= 2000)\n",
    "    X_train.comments.fillna(value = \"\", inplace=True)\n",
    "    X_counts = tfidf2.fit_transform(X_train.comments)\n",
    "    colnames = tfidf2.get_feature_names()\n",
    "    countsdf = pd.DataFrame(data= X_counts.toarray(), \n",
    "                            columns=colnames, \n",
    "                            index = X_train.index.values)\n",
    "    countsdf = countsdf.add_prefix('comments_')\n",
    "    X_train = X_train.join(countsdf)\n",
    "    X_train = X_train.drop('comments', axis =1)\n",
    "\n",
    "    X_test =  preprocessing(X_test)\n",
    "\n",
    "    # #tfidf on test\n",
    "    X_test.comments.fillna(value = \"\", inplace=True)\n",
    "    #the next line is just tranform for test set \n",
    "    #it was fit_transform for train\n",
    "    X_counts = tfidf2.transform(X_test.comments)\n",
    "    colnames = tfidf2.get_feature_names()\n",
    "    countsdf = pd.DataFrame(data= X_counts.toarray(),\n",
    "                            columns=colnames, \n",
    "                            index = X_test.index.values)\n",
    "    countsdf = countsdf.add_prefix('comments_')\n",
    "    X_test = X_test.join(countsdf)\n",
    "    X_test = X_test.drop('comments', axis =1)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    cv = GridSearchCV(model, {}).fit(X_train,Y_train )\n",
    "    # for col in train:\n",
    "    #      print col, np.any(train[col].isnull())\n",
    "\n",
    "    print(\"R Squared: {}\".format(cv.best_score_))\n",
    "\n",
    "    # Output the Mean Squared Error using our held out training data\n",
    "    mse = mean_squared_error(Y_test, cv.predict(X_test))\n",
    "    print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run this once crossval gives a good score.\n",
    "#If you changed something in the TFIDF, above you'll need to change it here\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "dropcolnames = list(set(train.columns) - set(test.columns))\n",
    "dropcolnames.extend(['quality', 'online'])\n",
    "Y = train['quality']\n",
    "train = train.drop(dropcolnames,axis = 1)\n",
    "test = test.drop('online', axis = 1)\n",
    "test_ids = test['id']\n",
    "\n",
    "train = preprocessing(train)  \n",
    "\n",
    "#tfidf on train\n",
    "tfidf = TfidfVectorizer( token_pattern=r'(?u)\\b[A-Za-z][A-Za-z]+\\b', max_features=1500)\n",
    "train.comments.fillna(value = \"\", inplace=True)\n",
    "X_counts = tfidf.fit_transform(train.comments)\n",
    "colnames = tfidf.get_feature_names()\n",
    "countsdf = pd.DataFrame(data= X_counts.toarray(), columns=colnames, index = train.index.values)\n",
    "countsdf = countsdf.add_prefix('comments_')\n",
    "train = train.join(countsdf)\n",
    "train = train.drop('comments', axis =1)\n",
    "\n",
    "test =  preprocessing(test)\n",
    "#tfidf on test\n",
    "test.comments.fillna(value = \"\", inplace=True)\n",
    "X_counts = tfidf.transform(test.comments)\n",
    "colnames = tfidf.get_feature_names()\n",
    "countsdf = pd.DataFrame(data= X_counts.toarray(), columns=colnames, index = test.index.values)\n",
    "countsdf = countsdf.add_prefix('comments_')\n",
    "test = test.join(countsdf)\n",
    "test = test.drop('comments', axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results.txt','w+') as opfile:\n",
    "    opfile.write(\"id,quality\\n\")\n",
    "    for i in range(test.shape[0]):\n",
    "#         print i\n",
    "        opfile.write(str(test_ids[i])+\",\"+str(prediction[i])+\"\\n\")\n",
    "\n",
    "opfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [rmp]",
   "language": "python",
   "name": "Python [rmp]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
