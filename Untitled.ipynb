{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#CrossVal\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold as KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit as SSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#GridSearch\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#ensembles\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "#text stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Just the preprocessing funtion. Play around with various features\n",
    "#I'm not using all the features yet\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = df.drop(['id', 'tid', 'date','dept' ], axis = 1)\n",
    "#      #One-hot forcredit\n",
    "#     df = df.join(pd.get_dummies(df.forcredit, prefix= 'forcredit'))\n",
    "    df = df.drop('forcredit', axis =1 )\n",
    "#     #One-hot attendance\n",
    "#     df = df.join(pd.get_dummies(df.attendance, prefix= 'attendance'))\n",
    "    df = df.drop('attendance', axis = 1)\n",
    "#      #One-hot textbookuse\n",
    "#     df = df.join(pd.get_dummies(df.textbookuse, prefix= 'textbk'))\n",
    "    df = df.drop('textbookuse' , axis =1)\n",
    "#      #One-hot interest\n",
    "    df = df.join(pd.get_dummies(df.interest, prefix= 'interest'))\n",
    "    df = df.drop('interest', axis =1)\n",
    "#      #One-hot grade\n",
    "    df = df.join(pd.get_dummies(df.grade, prefix= 'grade'))\n",
    "    df = df.drop('grade', axis =1)\n",
    "#      #One-hot profgender\n",
    "    df = df.join(pd.get_dummies(df.profgender, prefix= 'profgender'))\n",
    "    df = df.drop('profgender', axis =1)\n",
    "#      #One-hot profhotness\n",
    "#     df = df.join(pd.get_dummies(df.profhotness, prefix= 'profhotness'))\n",
    "    df = df.drop('profhotness', axis =1)\n",
    "#      #One-hot online\n",
    "#     df = df.join(pd.get_dummies(df.online, prefix= 'online'))\n",
    "#     df = df.drop('online', axis =1)\n",
    "     #One-hot tags\n",
    "#     df.tags = df.tags.apply(lambda x: '*'.join(eval(x)))\n",
    "#     df = df.join( df.tags.str.get_dummies(sep='*'))\n",
    "    df = df.drop('tags', axis =1)\n",
    "    \n",
    "    return df \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: 0.424074639208\n",
      "MSE: 3.9587335972\n"
     ]
    }
   ],
   "source": [
    "#CrossVal - I'm splitting the data into train and test, running the\n",
    "# preprocessing on all the training sets. Change the second number\n",
    "#in SSplit to control tehe number of iteration of crossval it should do\n",
    "#Try differnet models by changin the model variable\n",
    "\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "Y = data['quality']\n",
    "sss = SSplit(Y,1 ,test_size=0.4, random_state=7)\n",
    "ti =[]\n",
    "tti = []\n",
    "for train_index, test_index in sss:\n",
    "    ti = train_index\n",
    "    tti = test_index\n",
    "    X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    dropcolnames = []\n",
    "    dropcolnames.extend(['quality', 'online', \n",
    "                         'helpfulness', 'clarity', 'easiness'])\n",
    "\n",
    "    X_train = X_train.drop(dropcolnames,axis = 1)\n",
    "    X_test = X_test.drop(dropcolnames, axis = 1)\n",
    "\n",
    "    X_train = preprocessing(X_train)  \n",
    "\n",
    "\n",
    "    # #tfidf on train\n",
    "    tfidf2 = TfidfVectorizer(stop_words='english', \n",
    "                             token_pattern=r'(?u)\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                             max_features= 600)\n",
    "    X_train.comments.fillna(value = \"\", inplace=True)\n",
    "    X_counts = tfidf2.fit_transform(X_train.comments)\n",
    "    colnames = tfidf2.get_feature_names()\n",
    "    countsdf = pd.DataFrame(data= X_counts.toarray(), \n",
    "                            columns=colnames, \n",
    "                            index = X_train.index.values)\n",
    "    countsdf = countsdf.add_prefix('comments_')\n",
    "    X_train = X_train.join(countsdf)\n",
    "    X_train = X_train.drop('comments', axis =1)\n",
    "\n",
    "    X_test =  preprocessing(X_test)\n",
    "\n",
    "    # #tfidf on test\n",
    "    X_test.comments.fillna(value = \"\", inplace=True)\n",
    "    #the next line is just tranform for test set \n",
    "    #it was fit_transform for train\n",
    "    X_counts = tfidf2.transform(X_test.comments)\n",
    "    colnames = tfidf2.get_feature_names()\n",
    "    countsdf = pd.DataFrame(data= X_counts.toarray(),\n",
    "                            columns=colnames, \n",
    "                            index = X_test.index.values)\n",
    "    countsdf = countsdf.add_prefix('comments_')\n",
    "    X_test = X_test.join(countsdf)\n",
    "    X_test = X_test.drop('comments', axis =1)\n",
    "\n",
    "    model = ExtraTreesRegressor()\n",
    "    cv = GridSearchCV(model, {}).fit(X_train,Y_train )\n",
    "    # for col in train:\n",
    "    #      print col, np.any(train[col].isnull())\n",
    "\n",
    "    print(\"R Squared: {}\".format(cv.best_score_))\n",
    "\n",
    "    # Output the Mean Squared Error using our held out training data\n",
    "    mse = mean_squared_error(Y_test, cv.predict(X_test))\n",
    "    print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "dropcolnames = list(set(train.columns) - set(test.columns))\n",
    "dropcolnames.extend(['quality', 'online'])\n",
    "Y = train['quality']\n",
    "train = train.drop(dropcolnames,axis = 1)\n",
    "test = test.drop('online', axis = 1)\n",
    "test_ids = test['id']\n",
    "\n",
    "train = preprocessing(train)  \n",
    "\n",
    "#tfidf on train\n",
    "tfidf = TfidfVectorizer(stop_words='english', token_pattern=r'(?u)\\b[A-Za-z][A-Za-z]+\\b', max_features=300)\n",
    "train.comments.fillna(value = \"\", inplace=True)\n",
    "X_counts = tfidf.fit_transform(train.comments)\n",
    "colnames = tfidf.get_feature_names()\n",
    "countsdf = pd.DataFrame(data= X_counts.toarray(), columns=colnames, index = train.index.values)\n",
    "countsdf = countsdf.add_prefix('comments_')\n",
    "train = train.join(countsdf)\n",
    "train = train.drop('comments', axis =1)\n",
    "\n",
    "test =  preprocessing(test)\n",
    "#tfidf on test\n",
    "test.comments.fillna(value = \"\", inplace=True)\n",
    "X_counts = tfidf.transform(test.comments)\n",
    "colnames = tfidf.get_feature_names()\n",
    "countsdf = pd.DataFrame(data= X_counts.toarray(), columns=colnames, index = test.index.values)\n",
    "countsdf = countsdf.add_prefix('comments_')\n",
    "test = test.join(countsdf)\n",
    "test = test.drop('comments', axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "indices are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-c4b6ab0e0466>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mti\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/karthik/anaconda2/envs/rmp/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1989\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1991\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1992\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1993\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/karthik/anaconda2/envs/rmp/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2034\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2035\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2036\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2038\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/karthik/anaconda2/envs/rmp/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, convert, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   1631\u001b[0m         new_data = self._data.take(indices,\n\u001b[0;32m   1632\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m                                    convert=True, verify=True)\n\u001b[0m\u001b[0;32m   1634\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/karthik/anaconda2/envs/rmp/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   3700\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3702\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3704\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/karthik/anaconda2/envs/rmp/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n)\u001b[0m\n\u001b[0;32m   1854\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1856\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"indices are out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1857\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds"
     ]
    }
   ],
   "source": [
    "data[ti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results.txt','w+') as opfile:\n",
    "    opfile.write(\"id,quality\\n\")\n",
    "    for i in range(test.shape[0]):\n",
    "#         print i\n",
    "        opfile.write(str(test_ids[i])+\",\"+str(prediction[i])+\"\\n\")\n",
    "\n",
    "opfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [rmp]",
   "language": "python",
   "name": "Python [rmp]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
